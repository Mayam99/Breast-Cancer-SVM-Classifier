# -*- coding: utf-8 -*-
"""Breast-Cancer(SVM)Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JJQ27FI2b90gvoY3XtcKMoLFAEXowhwj

##Breast Cancer Diagnosis Using Support Vector Machine (SVM) with Hyper-Parameter Tuning
###In this project, we focus on building an efficient classification model for diagnosing breast cancer using the Breast Cancer Wisconsin (Diagnostic) Dataset from Kaggle. This dataset contains important diagnostic features extracted from cell nuclei images, which are crucial for distinguishing between benign and malignant cases of breast cancer.

###We will be implementing a Support Vector Machine (SVM) Classifier to effectively separate these two classes. SVM is well-suited for this classification task as it aims to find the optimal hyperplane to maximize the margin between classes, ensuring robust predictive performance.

###To enhance model accuracy and generalization, weâ€™ll perform hyper-parameter tuning on key parameters like kernel type, regularization (C), and kernel-specific parameters (e.g., gamma for RBF kernel). This approach allows us to identify the best configuration for our SVM model to achieve optimal performance on unseen data.

###This notebook includes:

* Exploratory Data Analysis (EDA) - Understanding data distribution, feature relationships, and class imbalances.
* Data Preprocessing - Scaling features and handling any missing values to ensure model consistency.
* Model Building and Tuning - Implementing SVM with hyper-parameter tuning using cross-validation.
* Evaluation - Using metrics like accuracy, precision, recall, and F1-score to assess model performance.

###This project showcases the effectiveness of SVM for medical diagnosis and highlights the critical role of tuning parameters to achieve the best results for accurate and reliable breast cancer prediction.
"""

# Data manipulation and visualization
import pandas as pd # It imports the pandas library and assigns it the alias "pd" for easier use.
import numpy as np # It brings in a tool called numpy, nicknamed "np", to help with number crunching in your code.
import matplotlib.pyplot as plt  # Import libraries for creating visualizations, aliased as plt.
import seaborn as sns # Import libraries for creating visualizations, aliased as sns.

from sklearn import datasets #Imports tools to load and use built-in or fetched datasets for machine learning tasks.
from sklearn.model_selection import train_test_split #Imports the function to split data into random train and test subsets for model evaluation.
from sklearn.preprocessing import StandardScaler #Imports the tool to standardize features by removing the mean and scaling to unit variance.
from sklearn.svm import SVC #Imports the Support Vector Classification model for building classification models.
from sklearn.metrics import accuracy_score #Imports the tool to calculate the accuracy of a classification model.

df = pd.read_csv('data (1).csv') # It reads the data from the CSV file named 'data (1).csv' and stores it in a Pandas DataFrame called 'df'.

df.head() # It displays the first 5 rows of the DataFrame df.

df.shape # It returns the dimensions of the DataFrame df as a tuple (number of rows, number of columns).

df.columns # It returns the names of all columns in the DataFrame df.

df.dtypes # It returns the data type of each column in the DataFrame df.

df.isnull().sum() # It returns the total number of missing values (NaN) in each column of the DataFrame df.

# Encode target variable
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})  # 1 = Malignant, 0 = Benign
# It replaces 'M' with 1 and 'B' with 0 in the 'diagnosis' column of the DataFrame df.

# Dropping non-informative columns like ID and any unrelated columns
df = df.drop(['id', 'Unnamed: 32'], axis=1)

df.head()

# Step 4: Splitting the Data into Training and Testing Sets
X = df.drop(['diagnosis'], axis=1)  # Features
y = df['diagnosis']                # Target variable
# These lines create the feature matrix X by dropping the 'diagnosis' column and the target variable y by selecting the 'diagnosis' column from the DataFrame df.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# It splits the features (X) and target (y) into training (80%) and testing (20%) sets using random sampling with a fixed seed for reproducibility.

# Step 5: Data Preprocessing - Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# These lines standardize the features in X_train and X_test using StandardScaler by removing the mean and scaling to unit variance.

# Step 6: Model Building - Initialize SVM with Hyper-Parameter Tuning
# Set up the hyper-parameter grid for tuning
param_grid = { # This dictionary is a key component in hyperparameter tuning.
    'C': [0.1, 1, 10, 100], #Regularization parameter, 'C': [0.1, 1, 10, 100] means that the tuning process will try these four different values for the regularization parameter C.
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1], # Kernel coefficient (influences the shape of the decision boundary).
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'] # Specifies the type of kernel used by the SVM (e.g., linear, radial basis function (RBF), polynomial).
}

from sklearn.model_selection import train_test_split, GridSearchCV

# Set up GridSearchCV for exhaustive search
svc = SVC() #  Creates an instance of the Support Vector Classifier (SVC) with default parameters. This will be the base model for the grid search.
grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)
# These lines automate the process of finding the optimal hyperparameters for the SVM model by trying out different combinations, evaluating them, and selecting the best one based on accuracy.
# This can significantly improve the model's predictive performance.

# Step 7: Get Best Parameters and Build Final Model
print("Best Parameters found:", grid_search.best_params_)
best_svc = grid_search.best_estimator_
# These lines help you identify and utilize the most effective SVM model configuration based on the results of the hyperparameter tuning process.
# The best_svc variable now holds your optimized SVM model, which can be used for making predictions on new data.

# Step 8: Model Evaluation on Test Set
y_pred = best_svc.predict(X_test)
# This line applies your best SVM model to the test data to generate predictions, which you can then compare to the actual target values (y_test) to evaluate the model's performance.

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Display Evaluation Metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for Breast Cancer Prediction')
plt.show()

